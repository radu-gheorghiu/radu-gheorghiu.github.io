{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"2016/12/10/how-to-use-a-sql-trigger-for-the-wrong-reason/","title":"How to use a SQL trigger for the wrong reason","text":"<p>A couple of days ago I came onto a question on Stackoverflow asking how to create a trigger to impose constraints. The question asked about a SQL trigger where the OP (original poster) asked for, and I quote:</p> <p>\u201d I understand the idea of triggers but a bit confused on how to use it to impose constraints.</p> <p>For example I have two tables: a student table and book_order table that shows what book a student orders. I want to create a trigger that will check that a student with a book order cannot be deleted from the student table.\u201d</p> <p>The request sounds simple and easily implementable with a SQL trigger, as the OP asked for, but it\u2019s wrong and just because you can it doesn\u2019t mean that you should. Although the question has gotten an answer which was also accepted, this solution is something which both I and others do not recommend.</p> <p>What the OP should implement in this type of scenario is a foreign key constraint on the book order table, referencing the student table, and that\u2019s it. That will perfectly fit the scenario which OP has described. All that he had to do is:</p> Create a foreign key constraint<pre><code>ALTER TABLE Book_Order\nADD CONSTRAINT FK_Student_Loaned_Books\nFOREIGN KEY (Student_ID)\nREFERENCES Student (ID);\n</code></pre> <p>Now, to detail a bit why a SQL trigger is not the better alternative.</p>","tags":["sql-server","data-engineering"]},{"location":"2016/12/10/how-to-use-a-sql-trigger-for-the-wrong-reason/#what-is-a-sql-trigger","title":"What is a SQL trigger?","text":"<p>A trigger is, as per Microsoft\u2019s latest version of the documentation (12-Dec-2016) : \u201cA trigger is a special kind of stored procedure that automatically executes when an event occurs in the database server. DML triggers execute when a user tries to modify data through a data manipulation language (DML) event. DML events are INSERT, UPDATE, or DELETE statements on a table or view. These triggers fire when any valid event is fired, regardless of whether or not any table rows are affected.\u201c</p> <p>From that definition we can see that by using a trigger, you trigger statements will fire each time there is a corresponding DML event on that table (in this case a DELETE), even if no rows are affected.</p> <p>So, that means that the SQL trigger logic will execute even if no rows might get affected. And depending on how you write the statements inside the trigger to check if there are other rows in the other table before deleting, then the process might be slower or less performant than implementing a FK constraint. If you\u2019re thinking that in both situations:</p> <ul> <li> <p>a DELETE from Student with FK constraint on Book_Orders</p> <p>OR</p> </li> <li> <p>a ON DELETE trigger that has a logic of checking if the student has any book loans</p> </li> </ul> <p>will hit the Book_Orders table, then you\u2019re right and below is an example of a execution plan for a DELETE with a FK constraint.</p> <p>I\u2019ve seen a lot of poorly written database code, especially code written in a procedural way rather than in a set based way or code that tries to bring in some advantages from OOP, but which do not work in a database.</p> <p>You\u2019re better off sticking with things that others have built and work well (that\u2019s OOP\u2019s code reusablity too, right?).</p> <p>This scenario is another example of that, where although you are able to write a piece of code, in a way you\u2019re thinking it will work, and it just might work too(!), but it doesn\u2019t mean that it\u2019s the right way.</p> <p>You might end up writing some non-performant T-SQL code in that your SQL trigger that will do a FULL TABLE SCAN of the Student table or even worse, a FULL TABLE SCAN of the Book_Order table, which I\u2019m expecting to be much bigger. And even if your database has a lot of RAM and can handle that, why not write good quality code that\u2019s like a fine made Swiss watch or like a well sharpened knife?</p> <p>SQL Server as well as other RDBMS systems have features in place that are very well implemented (by teams of experts) and implemented to work well with the entire database and although you can replace with \u201cyour-own-code goes here\u201d, often times you\u2019ll just end up beating the nail with the wrong end of a screwdriver (beating it with the handle).</p> <p>You\u2019ll get the job done, but in a way that you\u2019re consuming more energy, more resources and might even hurt yourself in the process.</p>","tags":["sql-server","data-engineering"]},{"location":"2016/12/16/how-to-find-queries-while-tracing-sql-server/","title":"How to find queries while tracing in SQL Server","text":"<p>Let\u2019s say you\u2019re working on a DEV database and you\u2019re trying to get some information from SQL Server Profiler about the number of READS or WRITES that your queries are making and / or CPU time spent or other metrics. On a database with a lot of users running queries on the DB at the same time this can be difficult. But luckily there\u2019s an easy fix to find those queries.</p> <p>So, if you\u2019re running queries from SSMS and want to identify your queries much easier in SQL Server Profiler you can filter the trace by Server Process ID (SPID). Now, in order to get this value let\u2019s remember that each tab in SSMS has its own session / process communicating with the DB, so each tab will have a different Server Process ID.</p> <p>So, to find out the corresponding SPID for a specific query tab all you need to do is:</p> <p>SELECT @@SPID</p> <p>Be mindful though, that this is available only for SQL Server 2008 and up.</p> <p>I hope this saves you a lot of time, it certainly did it for me, whenever I had to use SQL Server Profiler on a busy database.</p>","tags":[]},{"location":"2016/12/31/year-in-review-2016/","title":"My year in Big Data, ML and AI","text":"<p>Although I\u2019ve made this blog live a couple of months ago and it has a Big Data section, I didn\u2019t have the time to work with technologies from this field very much, especially in the latter part of the year.</p> <p>Even though I didn\u2019t manage to test these technologies and implement them in a \u201chomework\u201d project, I\u2019ve tried to keep myself up to date with things happening in this Big Data and Machine Learning ecosystem. And so, a small review of what small steps I managed to take this year:</p> <ul> <li>Made a few submissions to a Kaggle competition and dipped my feet into the \u201cdeep\u201d machine learning world</li> <li>Tried to keep up to date or at least be aware of the latest innovations in terms of machine learning from renowned universities and scholars:<ul> <li>arxiv.org</li> <li>Yann LeCun managed to convince me why neural nets are amazing and why they\u2019re going to be an important part of the future</li> <li>Andrej Karpathy tweeted about open.ai and got me excited about the first steps of democratizing modern AI</li> <li>Ian Goodfellow\u2018s idea of GAN\u2019s took off and generated a lot of positive debate</li> <li>Berkley\u2019s AmpLab honored the developments and contributions of its internal teams at the end of its project lifetime \u2013 just to mention Apache Spark which was both open-sourced and commercially set available through Databricks</li> <li>Hugo Larochelle made some very interesting streams via Periscope of some paper presentations from the ML ecosystem</li> <li>and many many others who have posted interesting papers on various approaches to machine learning</li> </ul> </li> <li>I found proof of a extremely interesting concept, which I knew was possible but never saw in a completed form, of computers being able to play games. This was in the form of MarI/O or  \u201cMachine Learning for Video Games\u201d as said by SethBling, the author of the video.</li> <li>Bots started to make some noise in the machine learning community and soon enough a few chat bots appeared in Facebook Messenger and will undoubtedly take off in the next year</li> <li>Google\u2019s Quickdraw showed that you don\u2019t have to fear a smart AI and that you can have loads of fun with it</li> <li>So many achievements and records have been set by AI systems where they were able to reach and surpass humans, like Microsoft\u2019s speech recognition achievement</li> <li>and many more, this is just off the top of my head</li> </ul> <p>Everything I\u2019ve just listed about this year\u2019s ML and AI advancements gets me very excited. And if these aren\u2019t sufficient, I\u2019m sure that AI in 2017 will have at least double the impact that it had on the world in 2016. Things will become smarter, more pervasive and as Kevin Kelly mentioned in his Ted Talk from June this year, it will eventually be the second industrial revolution.</p>","tags":[]},{"location":"2017/01/13/simplifying-like-conditions-in-where-clause/","title":"Simplifying LIKE conditions in WHERE clause","text":"<p>Let\u2019s say you have a query with a long WHERE clause in which you\u2019re checking if a set of characters exist in a column and the only way you\u2019re thinking of doing this is with a set of LIKE conditions in your where clause, like below:</p> Simplifying LIKE conditions in WHERE clause<pre><code>DECLARE @tbl TABLE (col1 VARCHAR(10));\n\nINSERT @tbl\nVALUES ('abc / 123')\n    , ('abc , 123')\n    , ('abc &amp; 123')\n    , ('abc : 123')\n    , ('abc ^ 123');\n\nSELECT *\nFROM @tbl\nWHERE col1 LIKE '%/%'\n    OR col1 LIKE '%,%'\n    OR col1 LIKE '%&amp;%'\n    OR col1 LIKE '%:%'\n    OR col1 LIKE '%^%';\n</code></pre> <p>But if you have to check for a large number of parameters then the query can get very big and it can look very messy.</p> <p>Thankfully, there\u2019s a way you can achieve the same logic as with multiple LIKE and OR conditions combined. All you have to do is to simplify the LIKE condition and put the characters you\u2019re searching for into square brackets in a single LIKE clause, just like in the following example.</p> The simplified LIKE condition<pre><code>SELECT *\nFROM @tbl\nWHERE col1 LIKE '%[/&amp;,^:]%';\n</code></pre>","tags":[]},{"location":"2017/02/04/watch-out-for-julia/","title":"Watch out for Julia","text":"<p>I first heard of Julia from Kaggle\u2019s \u201cFirst Steps With Julia\u201d competition and started looking around some of its features while on julialang.org and at that point I thought that it was maybe a replacement for R or even a more hardcore version of R developed especially for scientists.</p> <p>But recently I read a blog post from Microsoft\u2019s Cortana Intelligence and Machine Learning Blog on Julia \u2013 \u201cJulia \u2013 A Fresh Approach to Numerical Computing\u201d and I have to say that I feel like the tide will turn in a year or so and we will see a rapid adoption of Julia in the Machine Learning community.</p> <p>The blog post from Microsoft\u2019s blog was authored by one of Julia\u2019s co-creators, Viral B. Shah, who pointed one of Julia\u2019s major promises, the one to eliminate the so-called \u201ctwo language problem\u201d where you would use one high-level language for prototyping and drop down into C code in order to squeeze maximum performance for the production code.</p> <p>Julia overcomes this because there is no penalty in terms of performance for using either high-level or abstract constructs, essentially bridging the gap of coding differences between an engineer and a researcher.</p> <p>There are many more good things to be read about Julia, like GPU support for ML just to name one. I recommend reading the articles linked in this post for more information.</p>","tags":[]},{"location":"2017/02/07/data-science-azure-vm/","title":"Data Science Azure VM","text":"<p>It seems Microsoft is offering a complete ready to go software bundle for any Data Scientist in the form of a Azure Virtual Machine called the \u201cData Science Virtual Machine\u201c.</p> <p>Just looking at the list of pre-installed software, you have some of the common modern languages and frameworks already in the VM, so all you need to do is connect to your data source and start working:</p> <ul> <li>Microsoft R Server Developer Edition</li> <li>Anaconda Python distribution</li> <li>Julia Pro</li> <li>Jupyter notebook for R, Python, Julia</li> <li>Visual Studio with support for Python, R and node.js</li> <li>Microsoft\u2019s CNTK (Cognitive Toolkit) \u2013 open source deep learning framework</li> <li>mxnet</li> </ul> <p>and many others. So, maybe it\u2019s worth checking it out.</p>","tags":[]},{"location":"2017/03/02/split-string-in-sql-server-with-recursive-cte/","title":"Split string in SQL Server with Recursive CTE","text":"<p>I decided to spend my time productively while I was monitoring a database restore, so I wanted to see if I could create a recursive CTE that could do a split string in SQL Server into words (or tokens), based on a certain delimiter.</p> <p>And so I built the recursive CTE you can see next. Because I\u2019m not 100% sure if the order of the words will be consistent throughout multiple executions I also added a \u201cLVL\u201d column to enforce the order of the rows, so that you can be 100% certain that the way the result is displayed is the actual order of the words in the phrase.</p> Split string in SQL Server with Recursive CTE<pre><code>declare @string nvarchar(max) = 'the quick brown fox jumps over the lazy dog'\n , @delimiter nvarchar(10) = ' '\n\n;with cte as (\n    select @string phrase\n        , cast('' as nvarchar(max)) word\n        , 0 lvl\n\n    union all\n\n    select \n        ltrim(substring(phrase, charindex(@delimiter, phrase + ' ', 0 ), len(phrase))) phrase\n        , ltrim(substring(phrase, 0, charindex(@delimiter, phrase + ' ', 0 ))) word\n        , lvl + 1\n    from cte\n    where charindex(@delimiter, phrase + ' ', 0) &lt;&gt; 0\n    and len(phrase) &gt; 0)\nselect word\nfrom cte\nwhere len(word) &gt; 0\norder by lvl\n</code></pre> <p>As far as I have managed to test, this code should work on versions 2008R2 and up.</p> <p>On to the next item on the SQL Bucket list!</p>","tags":[]},{"location":"2017/03/09/finding-locked-tables-in-sql-server/","title":"Finding locked tables in SQL Server","text":"<p>Sometime this week I executed a DELETE query with an explicit BEGIN TRANSACTION on our DEV environment, the moved to another session to check some other results and forgot the transaction hanging. A few minutes later I left for home and by the time I was close to home a developer called me on my personal phone.</p> <p>He was very stressed saying that he can\u2019t do a <code>SELECT TOP 1 * FROM</code> OurBigTable and what can he do about it? It was at that point that I figured that I forgot the transaction open, without any COMMIT or ROLLBACK executed.</p> <p>I tried to guide him through executing sp_who2 to find my transaction but there were too many details (rows and columns) there and I couldn\u2019t remember the names of the relevant columns. So, I had to go back to work and sort things out, which I managed to fix in just 1 minute (find my session and kill it).</p> <p>Based on this, I decided to write a more comprehensive script that would allow my developers find locked tables in SQL Server in order to kill any transactions (on our DEV environment) that might cause problems such as these or at least more easily find the query / person who is locking a table and ask if they can kill the query or not.</p> Finding locked tables in SQL Server<pre><code>select locked_objects.*\nfrom\n    (select distinct\n        isnull(est.text, '- No query executing at the moment -')                                        [Query]\n        , db_name(tl.resource_database_id)                                                              [Database Name]\n        , object_name(tl.resource_associated_entity_id)                                                 [Object Name]\n        , tl.request_type                                                                               [Request Type]\n        , er.command                                                                                    [Query Type]\n        , er.status                                                                                     [Query Status]\n        , es.host_name                                                                                  [Host Name]\n        , es.program_name                                                                               [Program Name]\n        , es.login_name                                                                                 [Login Name]\n        , es.nt_user_name                                                                               [User Name]\n        , er.start_time                                                                                 [Query Start Time]\n        ,   cast(datediff(second, er.start_time, getdate()) / 60 as varchar(8)) + ' min ' +\n            cast(datediff(second, er.start_time, getdate()) % 60 as varchar(8)) + ' sec'                [Query Running For]\n        , er.percent_complete                                                                           [Query % Completed]\n        , er.transaction_id                                                                             [Transaction ID]\n        , case\n            when er.command = 'ROLLBACK TRANSACTION'\n                then ''\n            else N'Kill ' + cast(es.session_id as nvarchar(100)) + ';'\n          end  [Kill Command]\n    from sys.dm_tran_locks tl\n        inner join sys.dm_exec_sessions es on tl.request_session_id = es.session_id\n        left join sys.dm_exec_requests er on es.session_id = er.session_id\n        outer apply sys.dm_exec_sql_text (er.sql_handle) est\n    where 1 = 1\n        and tl.resource_type = 'OBJECT'\n        and right(tl.request_mode, 1) = 'X'\n        and tl.resource_associated_entity_id &lt;= 2147483647) locked_objects\ncross apply (   select name\n                from sys.objects\n                where OBJECT_ID(name, 'U') = OBJECT_ID(locked_objects.[Object Name], 'U')) checkObjects\nwhere 1 = 1\n    and checkObjects.name = 'tblB';     -- replace with table name HERE\n</code></pre> <p>This query will show you all the information that you need for this specific scenario to help you make the right decision. It shows you both what queries are running at the moment and locking the table or the locks that remained on a table after a query has been executed \u2013 probably an uncommitted transaction:</p> <p>And it also works for scenarios where a ROLLBACK is executing and keeping a resource locked, so you can see how much time it has been running for as well as the percentage of completion for that ROLLBACK operation so you will know in one glance approximately how much more you\u2019ll need to wait.</p> <p>Use it wisely!</p>","tags":[]},{"location":"2017/08/23/no-errors-raised-for-overflowing-data-when-using-char-and-varchar-data-types-in-sql-server/","title":"No errors raised for overflowing data when using CHAR and VARCHAR data types in SQL Server","text":"<p>It\u2019s not always possible to predict when problems will occur with your database\u2019s architecture and it\u2019s always a shock when it does, especially because most of the unexpected problems occur after a deploy to production (in this scenario I\u2019ve inherited the database, I wasn\u2019t the one to design it, or at least not this failing part).</p> <p>The latest problem I\u2019ve faced was with data which was 9 characters long attempted to be stored into a CHAR(8) column. Have a look and test the below script to see what happens:</p> No errors raised for overflowing data when using CHAR and VARCHAR data types in SQL Server<pre><code>declare @testInsertOverflow table (col varchar(3));\n\ninsert into @testInsertOverflow values ('1'), ('22'), ('333');\n\nselect *\nfrom @testInsertOverflow\n\ndeclare @x int = 123456;\n\ninsert into @testInsertOverflow\nselect @x;\n\ninsert into @testInsertOverflow (col) values (@x);\n\nselect *\nfrom @testInsertOverflow;\n</code></pre> <p>If you would have asked me before, I would have said that the 2nd and 3rd INSERT statements would fail since there would be an obvious truncation of data and we would get the infamous \u201cArithmetic overflow error converting expression to data type\u2026\u201d</p> <p>But you don\u2019t, and instead of throwing an error, SQL Server will insert rows with asterisks.</p> <p>This happens for the CHAR and VARCHAR datatypes, while NCHAR and NVARCHAR will throw the expected truncation error.</p> <p>It happens with all types of tables: variables, temporary and persisted and it happens even in *SQL Server 2016.</p> <p>Disclaimer:</p> <p>I\u2019m not sure if this issue is known or not, I couldn\u2019t really find any posts about this specific problem and that\u2019s why I decided to write this short article to then more easily share and maybe raise awareness and a red-flag about this problem so others can check they environments and plan accordingly, before the problem occurs.</p> <p>If anyone has experience with this or knowledge about why this happens I\u2019d really appreciate it if you could add a comment.</p>","tags":[]},{"location":"2023/12/31/year-in-review-2023/","title":"2023 - A year in review","text":"<p>On the 30th of November 2022, I created my ChatGPT account and started trying out the capabilities of ChatGPT, which back then represented a major disruption of what we knew as chatbots. I remember trying out in the first days the capabilities of the chatbot, mainly focusing on what we would now call \"reasoning\" capabilites. I tested out its flexibility, it's understanding of instructions and its ability to accomplish tasks.</p> <p>But having a chatbot that could more directly reply to your questions, in natural language, did not feel like a big enough breakthrough that could change the world. It was a nice to have, but it was not a must have. It was not a game changer.</p> <p>Because most of our world runs in code, I wanted to see what place I could find for this technology, in the world of software development. I wanted to see if I could use it to make my life easier, to make my work more efficient, to make my work more enjoyable.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#a-spark-of-potential","title":"A spark of potential","text":"<p>I remember when I gave the first instructions to ChatGPT to process a certain text and respond in a certain way, using a JSON schema which I had gave it and instructed it to fill in the appropriate values.</p> <p>I want you to process this text and reply with the following JSON schema? The text is: \"My name is John and I am 30 years old. I live in New York, on 5th Avenue, number 10.\". The JSON schema is:</p> <pre><code>{\n    \"name\": &lt;name&gt;,\n    \"age\": &lt;age&gt;,\n    \"address\": {\n        \"street\": &lt;street address&gt;,\n        \"number\": &lt;street number&gt;,\n        \"city\": &lt;city&gt;,\n        \"country\": &lt;country&gt;\n    }\n}\n</code></pre> <p>The instructions felt \"sloppy\" and unorganized, requirements not clearly defined, as we were used to in the world of software development. But the chatbot was able to understand what I wanted and to deliver the results I was expecting. The chatbot was able to deliver the results I was expecting, without any problems, without any errors, without any delays.</p> <pre><code>{\n    \"name\": \"John\",\n    \"age\": 30,\n    \"address\": {\n        \"street\": \"5th Avenue\",\n        \"number\": 10,\n        \"city\": \"New York\",\n        \"country\": \"USA\"\n    }\n}\n</code></pre> <p>That is when I could feel a change that would happen. I started fiddling with the chatbot even more, to find its strengths and its weaknesses. Many people were saying that the bot is a great replacement for Google and doing a search, but that its reasoning capabilities are not that great, that we are still far away from such a technology to replace human reasoning.</p> <p>But just because it wasn't able to do something you expected, when you gave it short instructions, wasn't something that convinced me of its inability to \"reason\". The JSON examples proved me wrong, it gave me a sense of what we would end up calling \"prompt engineering\". I discovered on my own that if you do ask the bot the right question using the proper wording and instructions, it would be able to do what you wanted it to do.</p> <p>I remember people asking the bot for code and it would sometimes be a hit or miss, thus ending up by discrediting it completely and saying that it is not a good replacement for a software developer. But I knew that it was just a matter of time until we would be able to use it to write code for us. I felt that we just need to find a way to ask the bot to give us the right information or for the tecnology to make a small incremental improvement and we would get better models, better chatbots, better results. I saw a spark of potential and decided to dive deeper into the next opportunity that would come my way.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#the-hackathons","title":"The hackathons","text":"<p>In late December, I stumbled upon an Instagram post that mentioned an OpenAI hackathon. I digged a bit deeper into this and noticed that lablab.ai was organizing a hackathon using the OpenAI API, in March 2023. I was excited about the opportunity and decided to sign up for it and for 2 other hackathons, which were before it, but had the same focus of building applications using LLMs.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#1-ai21labs-hackathon-kitchengenie","title":"1. AI21Labs Hackathon - KitchenGenie","text":"<p>Between the 13th and 20th of January, me and Aurel Godoroja formed the \"LastMinute\" team and signed up for the hackathon. We spent the first 3 days of the hackathon trying out the models, trying to understand the difference between an Instruct model and a simple Generate model, find out the strengths and weaknesses of each model and see what we could do with them.</p> <p>I started looking online for ideas that we could use the model for, since we were not able to come up with a good idea which excited us both. At the time I was looking for some Upwork jobs so one evening I took a break and looked over the list of jobs. There was someone who was asking for an app that could take an order for a pizza, from a customer, in natural language and calculate the price of the pizza, based on the description of the pizza from the customer.</p> <p>Things started to click in my head. I thought, why not a recipe generator, an app that can help you with your recipes, I know I had a ton of problems with making recipes and not letting the food spoil in my fridge. I shared my idea with Aurel and he liked it and since we had only about 3 days left in the competition, we started hacking away. We built the application so that it would simulate our bigger idea, which was to have a phone app that you could take photos of the food in your fridge to identify ingredients. Then you would pick the ingredients you want and use the app to generate a recipe for you, based on the ingredients you have in your fridge.</p> <p>We gave it our all, both working full time jobs during the day and during the nights, hacking on the project. Neither of us has significant frontend experience so we decided to go with Streamlit which seemed at that time could solve all of our requirements for a user interface. The next days would prove us wrong and we spent countless hours trying to fix issues with caching variables and states so that we we interacted with the UI we wouldn't lose the flow of the application. We both ended up sleeping only 2-3 hours per night and in the last night before the submission, neither of us would get any sleep. We were absolutely all in and exhausted, but we didn't know how far we pushed ourselves.</p> <p>We ended up finishing and making our submission for the hackathon with only 30 seconds to spare, but we were in for the judging!</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#on-the-stage","title":"On the stage","text":"<p>We were selected as one of the finalists and were invited to present our project during the Winner's announcement livestream.</p> <p>Though we weren't the winner, being one of the runner ups meant a lot to us, especially since it was our first hackathon and we were building something with a technology that we were still learning about. This experience boosted our morale for the next hackathon and we were excited to see what we could do next.</p> <p>Although we got close to winning, we felt that we'd won, but the coming days would show us what toll the hackathon took on us. I lost my appetite for the next 2 days and had difficulties falling asleep for the next week. Aurel also had health problems we didn't think much of back then</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#2-cohere-hackathon-tubetalk","title":"2. Cohere Hackathon - TubeTalk","text":"<p>Our second hackathon with lablab.ai was the Cohere Hackathon, which took place between the 27th of January and the 3rd of February.</p> <p>This time though, we unfortunately struggled a bit more with finding an interesting idea, the intuition of working with LLMs in the first hackathon didn't replicate when working with Cohere's API and we struggled a bit more.</p> <p>We ended up builidng an application that integrated with YouTube and analized thousands of comments from a video and generated a summary of the viewer's feedback about your video. We called it TubeTalk.</p> <p>Just like the last hackathon, we worked full time jobs during the day and hacked on the project during the night. We ended up sleeping very few hours in the first days of the hackathon and we were both exhausted. We ended up submitting the project with only 30 minutes to spare, but we were happy it was over.</p> <p>This time we didn't make it to the finals, but we were happy that we were able to finish the project and submit it on time. We were also happy that we were able to get some sleep and rest a bit. But it's only when we started to notice what toll two hackathons in a row took on us, along with working full-time jobs and one of us also had a side-project and a client he was assisting.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#we-needed-a-break","title":"We needed a break","text":"<p>Unfortunately, when the OpenAI hackathon came, we were both exhausted and we didn't have the energy to participate in it. We were both disappointed that we couldn't participate in it, but we knew that we needed to take a break and rest a bit. We both had serious health issues, tests came out showing that we had quite a few problems and we needed to make some changes and take some rest, take care of ourselves.</p> <p>I had some problems which took me a few months to recover from so I could just do my full-time job and then I could do no more. I struggled to find the energy to do some extra reading on the topic of LLMs, to keep in touch with what the community would build and what are the developments.</p> <p>However, even though I was out for a while the e/acc movement was pushing for more development and more progress in this technology. It seems like nothing is going to stop progress in this space and the community will keep pushing forward.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#getting-back-on-track-learnacc","title":"Getting back on track - learn/acc","text":"<p>Once I got healthy enough to get back into the space, I had already been following the courses that Databricks announced they would do in order to teach people how to work with LLMs.  And so starting in early June, I took part in the first cohort of students that would follow their two-part LLM course series.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#llm101x-llms-application-through-production","title":"LLM101x: LLM's - Application through Production","text":"<p>During this first course, the \"LLM101x: LLM's - Application through Production\" I learned how to:</p> <ul> <li>create and deploy Large Language Models (LLMs)</li> <li>started from the basics of Natural Language Processing (NLP) all the way to cutting-edge applications</li> </ul> <p>The course was designed for technical roles like mine and covered a broad spectrum, including the emerging field of LLMOps and the societal impact of LLMs.</p> <p>I got hands-on experience with the Databricks environment and the Hugging Face library. </p> <p>Specifically, I dived into:</p> <ul> <li>foundational NLP tasks</li> <li>vector databases</li> <li>multi-stage reasoning</li> <li>fine-tuning</li> <li>model selection</li> <li>model evaluation</li> </ul> LLM101x certificate of completion","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#llm102x-llms-foundation-models-from-the-ground-up","title":"LLM102x: LLM's - Foundation Models from the Ground Up","text":"<p>In the second course, \"LLM102x: LLM's - Foundation Models from the Ground Up\" I went beyond the surface to understand the inner workings of Large Language Models (LLMs). Aimed at technical practitioners and scientists, the course gave me a strong technical and theoretical foundation.</p> <p>Starting from the basics like the transformer block and attention mechanism, it moved to advanced topics like multi-modal LLMs. I learned about parameter-efficient fine-tuning (PEFT), quantization, and Mixture-of-Experts (MoE) approaches for efficient deployment.</p> <p>The course also gave me the chance to deepen my skills in PyTorch and get hands-on experience with the Databricks environment and Hugging Face platform.</p> <p>Specific topics covered included:</p> <ul> <li>the transformer architecture</li> <li>efficient fine-tuning</li> <li>optimizing deployment and training for LLMs</li> </ul> LLM102x certificate of completion","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#keeping-up-with-the-community","title":"Keeping up with the community","text":"<p>Even though I couldn't spend much time in actually coding, I still wanted to keep up with the community and that is when I started diving into Twitter more, until I found @swyx, who was one of the most enthusiastic and active people that I have ever seen. He shared tons of ideas of new concepts and roles for software engineers, he defined the first roles and responsibilites of the new \"AI Engineer\", in his Latent Space post, \"The Rise of the AI Engineer\".</p> <p>During the summer months I expanded my Twitterverse and started following more and more people in the space, some who are more vocal, some who are more technical, some who are more business oriented. I started to get a sense of what the community is building, what are the new developments, what are the new ideas and what are the new opportunities.</p> <p>Even when going on vacation and at weddings in August and September, I was still reading articles about the new architectures that are going to replace the Transformers models, new techniques for prompting like Chain-of-Thought (CoT) and Tree-of-Thought.</p> <p>When everyone's wish of longer model contexts was delivered through Anthropic's 100k Claude model, we hit a new problem, the Lost-in-the-Middle problem, explored and detailed in this article.</p> <p>It seems that although the technology was growing in numbers, it wasn't also growing in quality and usefulness. The models were getting bigger and bigger, but the results were not getting better and better. The models were getting more and more expensive to train, but the results were not getting more and more useful.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2023/12/31/year-in-review-2023/#the-end-of-the-beginning","title":"The end of the beginning","text":"<p>During the last part of the year, starting around October, I observed that the open-source community started to come closer and closer to the closed-source models. New models, smaller models, provided most of the same benefits as larger models, but at a fraction of the cost. Also, inspired by @abacaj's AI rig, I decided to look into buying my own hardware to start experimenting locally, first with fine-tuning and maybe in the future with training smaller models.</p> <p>At the beginning of December, after a few weeks of research on what is the optimal hardware for my needs, but something that I could also scale up into the future, I pulled the trigger on an RTX 3090 Founder's Edition. I had to make sure that the GPU I get is compatible with the workstation I already have at home, so that I can start experimenting and growing my knowledge of working with OSS and local LLMs.</p> It's messy, I know, but it works <p>Although 2023 was a big year for the machine learning space, it is only the beginning. Anyone who has just started, we are still early. </p> <p>We've only just scratched the surface of working with multimodal models like GPT4-V, we have yet to see the full potential of small models working together in solving tasks, we've yet to expand the Agents architectures and combine them with more effective prompting techniques. We're still early and if you're a software developer, you would better start learning and experimenting with these technologies, because they will change the world.</p>","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"2024/12/31/year-in-review-2024/","title":"2024 - A year in review","text":"","tags":["generative models","chatbots","prompt engineering","open-source"]},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"archive/2016/","title":"2016","text":""},{"location":"category/year-in-review/","title":"Year in Review","text":""},{"location":"category/ai/","title":"AI","text":""},{"location":"category/data-engineering/","title":"Data Engineering","text":""},{"location":"category/sql-server/","title":"SQL Server","text":""},{"location":"category/machine-learning/","title":"Machine Learning","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"category/data-engineering/page/2/","title":"Data Engineering","text":""}]}